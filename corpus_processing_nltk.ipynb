{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus preparation and processing\n",
    "\n",
    "This notebook will show you how to use [NLTK](https://www.nltk.org/) to prepare your corpus for processing. \n",
    "\n",
    "You can run this notebook online using something like [Google Collab](https://colab.research.google.com/), but you can also download it and run it locally using [Jupyter Notebooks](https://jupyter.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing NLTK\n",
    "\n",
    "The next 3 lines install NLTK, import it into this notebook, and then tell NLTK to download all the components. If running this notebook locally, you'll only have to do the first and the last lines once. When you have done it once, you can comment them out.\n",
    "\n",
    "After you have installed NLTK and downloaded the components, the only thing you'll have to do in the future is import it on any notebook you want to use it in, by running the middle line (`import nltk`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading files\n",
    "\n",
    "The processes below are just for a single file at a time. You can do this for an entire directory as well (see below). In the code below, you will have to change the text after `file` to the path where your file is (in my case, `C:/Maite/MOD/notebooks/Ling810/data/`) and to the name of the file (`CBC_446.txt`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file='C:/Maite/MOD/notebooks/Ling810/data/CBC_446.txt', mode='r', encoding='utf-8') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line below just shows you the contents of the variable `text`. You will see lines like this after each of the processing steps. They are just so that you can see what happened to the text. You don't have to run them every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "The file was read into a variable called `text`. We need to ask NLTK to tokenize it into words and punctuation. We first import the right function and then tokenize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus processing\n",
    "\n",
    "There are several things you can do with the data, depending on your goals:\n",
    "\n",
    "* Make all the words lowercase\n",
    "* Remove all punctuation\n",
    "* Remove stop words\n",
    "* Lemmatize\n",
    "\n",
    "You may do some or all of them, or you may do them in different orders. Here, I just show you how to do one at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowercase\n",
    "\n",
    "This is useful if you want to count all instances of the same word, regardless of where they appear in the text (\"The\" and \"the\"). But beware that when you do this, you may count instances of \"Mark\" (the person's name) and \"mark\" (a grade) as instances of the same word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_tokens = [token.lower() for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_words =[token for token in tokens if token.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that I did the remove punctuation operation on `tokens`, which contains the text before lowercasing. If you want to remove punctuation from the lowercased text, then you change from `tokens` to `lower_tokens`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_words_lower =[token for token in lower_tokens if token.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_words_lower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stopwords\n",
    "\n",
    "Stopwords are lists of words that you do not want to include in your corpus, perhaps because you don't want to count function words and other common words. We will be using NLTK's standard list of English stopwords, but you can define your own too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    " \n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_no_stopwords = [token for token in tokens if token not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_no_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, I performed this operation on all the tokens (`tokens`). I could also do it for `only_words` or `only_words_lower`, which has all the punctuation removed and the text in lowercase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_words_lower_no_stopwords = [token for token in only_words_lower if token not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_words_lower_no_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize\n",
    "\n",
    "There are different ways of lemmatizing. This uses the [WordNet](https://wordnet.princeton.edu/) dictionary to find the lemma for the word. This means not all words will be found. There are better lemmatizers, including the functions in [spaCy](https://spacy.io/), if you want to explore those. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    " \n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_words = [lemmatizer.lemmatize(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_words_lower = [lemmatizer.lemmatize(token) for token in only_words_lower_no_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_words_lower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process an entire directory\n",
    "\n",
    "The steps above are for one file at a time. If you have a directory of .txt files, you can process all of them and save the output to a new set of files, tokenized. Remember that, to do this, you need to import NLTK and the tokenization, if you haven't run the relevant lines of this notebook above. All of the import statements and global definitions are repeated here, just in case you only want to run this portion. You'll also need the `os` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stopwords = stopwords.words('english')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the path to the directory where your files are. This will save the output to a subdirectory there, but you can define the output directory to be whatever you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'C:/Maite/MOD/teaching/Ling810_Fall2024/data/CBC_Dec2023/CBC_Dec2023_sample20'\n",
    "output = 'C:/Maite/MOD/teaching/Ling810_Fall2024/data/CBC_Dec2023/CBC_Dec2023_sample20/processed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the function that is going to do all the work. This only works for txt files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_files(directory):\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.txt'):  \n",
    "            file_path = os.path.join(directory, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                # tokenize\n",
    "                tokens = word_tokenize(text)\n",
    "                # prepare the path to the output dir and relevant file\n",
    "                output_file_path = os.path.join(output, f'tokenized_{filename}')\n",
    "                \n",
    "                with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "                    out_file.write(' '.join(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now call the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_files(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That function only tokenizes. You can from there also lowercase, remove punctuation, etc. If you want to do the whole process (tokenize, remove punctuation, remove capitalization, lemmatize), you can do it all in one function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files(directory):\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.txt'):  \n",
    "            file_path = os.path.join(directory, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                # tokenize\n",
    "                tokens = word_tokenize(text)\n",
    "                # lowercase and remove punctuation\n",
    "                cleaned = [token for token in tokens if token.lower() not in stopwords and token.isalpha()]\n",
    "                # lemmatize\n",
    "                lemmatized = [lemmatizer.lemmatize(token) for token in cleaned]\n",
    "                # prepare output path \n",
    "                output_file_path = os.path.join(output, f'processed{filename}')\n",
    "                \n",
    "                with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "                    out_file.write(' '.join(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now call this other function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_files(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
