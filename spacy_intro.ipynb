{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy and POS tagging\n",
    "\n",
    "For more info: [spaCy](https://spacy.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing spaCy and language model\n",
    "\n",
    "If running this notebook locally, you'll only have to do the next two lines once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aside: two ways of installing spaCy\n",
    "\n",
    "### 1. Jupyter notebook\n",
    "You can install it the way you see above, by running the 2 lines above in your Jupyter notebook. Remember you only have to do this once. After you run this notebook on your local machine, you can comment those two lines out.\n",
    "\n",
    "### 2. Command prompt\n",
    "* In Windows, if you have Ananconda, you can open an Anaconda powershell prompt. If you don't have Anaconda, just open a Windows powershell in admin mode.\n",
    "* On a Mac, open a terminal window (spotlight, \"terminal\"). Or look for \"terminal\" in your apps folder. \n",
    "\n",
    "Now that you have a command window open, simply go to the spaCy website and choose your operating system to copy and paste the right commands (one at a time). Click on the right options for you from here: https://spacy.io/usage  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading spaCy and language model\n",
    "Installation (if local) only needs to be done once. However, you need to import the spaCy module and load the language model every time you want to use it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining some sentences\n",
    "\n",
    "Remember that you can call your variables what you want. Play with the names of the variables and the contents of the sentences below. You don't need to print the sentence each time; I've only done it for the first so that you see it's stored in the right variable. Note the difference between `print(sent1)` and `sent1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = \"The motivational speaker exhorted us to change the way we live today, rather than looking always toward some vague distant futurity.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from a CBC story, https://www.cbc.ca/news/canada/nova-scotia/loblaws-will-no-longer-offer-50-discount-on-expiring-food-products-1.7084299\n",
    "sent2 = \"According to an email from Loblaw Companies Ltd. reviewed by CBC News, it will no longer discount perishable foods like meat, fruit, and vegetables, by 50 per cent off as they near their expiration date.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting string to doc with spaCy\n",
    "spaCy has a special type of object, a `Doc`. It's the entire processing pipeline for any NLP system, in a single object. It takes a text, e.g., `sent1` and applies all the NLP steps to it (tokenization, tagging, named entity recognition). Once you have converted a string (a sentence) or a whole text to Doc, you can access everything that spaCy has done with it, i.e., the entire structure of language information that it has applied to it, with labels. spaCy refers to that language information and labels as 'linguistic annotations'. spaCy does this with a simple function, `nlp()`.\n",
    "\n",
    "![spaCy pipeline](https://spacy.io/images/pipeline.svg)\n",
    "\n",
    "Image from https://spacy.io/usage/processing-pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(sent1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`doc1` now contains a complex python data type. Note that the ouput of just calling the variable (`doc1`) is slightly different than when we call a string variable, above (`sent1`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accesing the information in the Doc object\n",
    "\n",
    "`doc1` contains lots of [useful information](https://spacy.io/api/doc):\n",
    "\n",
    "* tokens (words)\n",
    "* lemmas\n",
    "* morphology\n",
    "* POS tags\n",
    "* syntactic structure (a parse tree)\n",
    "* named entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print word tokens\n",
    "\n",
    "for token in doc1:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmas\n",
    "\n",
    "for token in doc1:\n",
    "    print(token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# morphology\n",
    "\n",
    "for token in doc1:\n",
    "    print(token.text, token.morph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tags (more on this below)\n",
    "\n",
    "for token in doc1:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# syntactic structure, as dependencies\n",
    "\n",
    "for token in doc1:\n",
    "    print(token.i, token, token.dep_, token.head.i, token.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# named entities\n",
    "\n",
    "for ent in doc1.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some of the same things, but for `sent2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp(sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc2:\n",
    "    print(token.text, token.morph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc2:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc2.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prettier visualizations\n",
    "\n",
    "`displacy` is another module that allows us to see dependencies and entities in a user-friendly way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# syntactic structure, dependency\n",
    "displacy.render(doc1, style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can change the look of the dependency tree\n",
    "options_parse = {\"compact\": True, \"bg\": \"#f3f4f5\",\n",
    "           \"color\": \"black\", \"font\": \"Source Sans Pro\"}\n",
    "\n",
    "displacy.render(doc1, style=\"dep\", options=options_parse, jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can change the look of the dependency tree\n",
    "options_parse = {\"compact\": True, \"bg\": \"#f3f4f5\",\n",
    "           \"color\": \"black\", \"font\": \"Source Sans Pro\"}\n",
    "\n",
    "displacy.render(doc2, style=\"dep\", options=options_parse, jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc1, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc2, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Practice!\n",
    "\n",
    "Play with different sentences. You can also do more than one sentence in one `sent` string. And you can try all of the above with a longer text. You can also test how well it detects sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tags and how to examine them\n",
    "\n",
    "Let's look a bit more at part of speech tags. The POS tagger in spaCy produces [a lot of information](https://spacy.io/api/token) and you can query it in different ways. \n",
    "\n",
    "The first bit of code gives you most of the information attached to each word. Note the difference between `token.pos_` and `token.tag_`. The first gives you a high-level tag (DET, NOUN, VERB) from the [Universal POS tagset](https://universaldependencies.org/u/pos/). The second gives you a more specific subtype of that (NN = singular noun, VBD = verb in the past tense, VBG = verb in the gerund). \n",
    "\n",
    "If you don't know what a tag means, you can ask spaCy to explain: `spacy.explain()` (next few code blocks).\n",
    "\n",
    "You can also query spaCy for all its tags (last code block below). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc1:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explain a specific tag\n",
    "\n",
    "spacy.explain(\"NN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explain a specific tag\n",
    "\n",
    "spacy.explain(\"VBD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explain a specific tag\n",
    "\n",
    "spacy.explain(\"VBG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explain all the labels spaCy produces\n",
    "\n",
    "for label in nlp.get_pipe(\"tagger\").labels:\n",
    "    print(label, \" -- \", spacy.explain(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A few more things about POS tags\n",
    "\n",
    "You can list all the words that have a specific POS in the sentence/text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Nouns:\", [token.lemma_ for token in doc2 if token.pos_ == \"NOUN\"])\n",
    "print(\"Verbs:\", [token.lemma_ for token in doc2 if token.pos_ == \"VERB\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can list all the phrases in a sentence (which is really part of the dependency structure, not the POS tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Noun phrases:\", [chunk.text for chunk in doc1.noun_chunks])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can list the dependency structure of the sentence, by asking what the dependency relation is for each word, and what its head is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc1:\n",
    "    print(token.text, token.tag_, token.dep_, token.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test ambiguities\n",
    "\n",
    "The lecture notes had examples of ambiguous sentences or words that could have multiple tags. Try some of those and see how well spaCy tags them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_amb = \"That is the back door. He was lying on his back. We want to win the voters back. She promised to back the bill. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_amb = nlp(sents_amb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc_amb:\n",
    "    print(token.text, token.tag_, token.dep_, token.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: making it all easier to ready\n",
    "\n",
    "This is optional and you don't need to know pandas yet, but, in case you want to make the output a bit easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = []\n",
    "\n",
    "for token in doc1:\n",
    "    data1.append([token.text, token.tag_, token.dep_, token.head])\n",
    "    \n",
    "df = pd.DataFrame(data1)\n",
    "df.columns = ['Text', 'Tag', 'Dependency', 'Head']\n",
    "\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
